{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9411bf0d",
   "metadata": {},
   "source": [
    "  # Frequent Itemset Generation Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b036e471",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7389e4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f168c6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95a9c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import fpgrowth\n",
    "from mlxtend.frequent_patterns import association_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1225df11",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r predict_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a1d745",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_df = predict_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f770ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predict_df.loc[14, 'content'])\n",
    "print(predict_df.loc[14, 'clean_content'])\n",
    "predict_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603ebfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_appwords():\n",
    "    stpwrds = [\"app\", \"alexa\", \"facebook\", \n",
    "                     \"googlehome\", \"instagram\", \"linkedin\", \"tiktok\", \"tik\", \"tok\", \"uber\", \"youtube\", \"fb\", \n",
    "               \"dont\", \"yall\", \"kinda\", \"lot\", \"anymore\", \"doesnt\", \"tube\", \"blm\", \"thing\"]\n",
    "    return stpwrds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1445774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text, stpwrds):\n",
    "    #text = text.split(\" \")\n",
    "    words = [w for w in text if w not in stpwrds]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb41558",
   "metadata": {},
   "outputs": [],
   "source": [
    "stpwrds = generate_appwords()\n",
    "predict_df['clean_content'] = predict_df['clean_content'].apply(lambda x: remove_stopwords(x, stpwrds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93188c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_df['clean_content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c35be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dictionary to store the sub dataframes for each app\n",
    "app_subdfs = {}\n",
    "\n",
    "# Iterate over each unique app name\n",
    "for app in predict_df['app_name'].unique():\n",
    "    # Filter the dataframe for the current app\n",
    "    sub_df = predict_df[predict_df['app_name'] == app]\n",
    "    # Store the sub dataframe in the dictionary with the app name as the key\n",
    "    app_subdfs[app] = sub_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55326783",
   "metadata": {},
   "source": [
    "### One-hot encoding transaction data\n",
    "corpus_list here transformed into a one-hot encoded data frame, where each column consists of true and false values that indicate whether a word was included in a review. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2c2249",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_corpus_list = {}\n",
    "def create_corpus(df,  concern: bool = False):\n",
    "    for app in app_subdfs.keys():\n",
    "        if concern:\n",
    "            for index, row in app_subdfs[app].iterrows():\n",
    "                row[\"clean_content\"].append(row[\"predicted\"])\n",
    "        corpus_list = app_subdfs[app][\"clean_content\"].tolist()\n",
    "        app_corpus_list[app] = corpus_list\n",
    "    return app_corpus_list\n",
    "\n",
    "app_corpus_list = create_corpus(app_subdfs, concern=True)\n",
    "#app_corpus_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1780c96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dictionary to store the sub dataframes for each app\n",
    "app_corpus_subdfs = {}\n",
    "te = TransactionEncoder()\n",
    "for app in app_subdfs.keys():\n",
    "    print(app)\n",
    "    te_ary = te.fit(app_corpus_list[app]).transform(app_corpus_list[app])\n",
    "    corpus_df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "    app_corpus_subdfs[app] = corpus_df\n",
    "    print(corpus_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbb5e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a list to store the number of unique words for each app\n",
    "unique_words_app = {}\n",
    "\n",
    "# Iterate over the sub-dataframes\n",
    "for app, sub_df in app_corpus_subdfs.items():\n",
    "    unique_words_app[app] = len(sub_df.columns)\n",
    "unique_words_app = sorted(unique_words_app.items(), key=lambda x: x[1], reverse=True)\n",
    "unique_words_app = dict(unique_words_app)\n",
    "# Sort the unique_words_app dictionary by values in descending order\n",
    "unique_words_app_list = sorted(unique_words_app.items(), key=lambda x: x[1], reverse=False)\n",
    "\n",
    "# Create a list to store the number of reviews for each app\n",
    "no_review_app = {}\n",
    "\n",
    "# Iterate over the sub-dataframes\n",
    "for app, sub_df in app_corpus_subdfs.items():\n",
    "    no_review_app[app] = len(sub_df.index)\n",
    "no_review_app = sorted(no_review_app.items(), key=lambda x: x[1], reverse=True)\n",
    "no_review_app = dict(no_review_app)\n",
    "# Sort the no_review_app dictionary by values in descending order\n",
    "no_review_app_list = sorted(no_review_app.items(), key=lambda x: x[1], reverse=False)\n",
    "\n",
    "# Extract the sorted apps and counts for unique words\n",
    "apps_unique_words = [app for app, count in unique_words_app_list]\n",
    "counts_unique_words = [count for app, count in unique_words_app_list]\n",
    "\n",
    "# Extract the sorted apps and counts for reviews\n",
    "apps_reviews = [app for app, count in no_review_app_list]\n",
    "counts_reviews = [count for app, count in no_review_app_list]\n",
    "\n",
    "# Set the width of the bars\n",
    "bar_width = 0.45\n",
    "\n",
    "# Create an array of indices for the x-axis ticks\n",
    "ind = np.arange(len(apps_unique_words))\n",
    "\n",
    "# Create the figure and axes\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "# Plot the bars for unique words\n",
    "bars_unique_words = ax.barh(ind, counts_unique_words, bar_width, color='#1f77b4', label='Unique Words')\n",
    "\n",
    "# Plot the bars for reviews\n",
    "bars_reviews = ax.barh(ind + bar_width, counts_reviews, bar_width, color='#0f3652', label='Reviews')\n",
    "\n",
    "# Add value labels to each bar for unique words\n",
    "for count, bar in zip(counts_unique_words, bars_unique_words):\n",
    "    ax.text(bar.get_width() + 48 , bar.get_y() + bar.get_height() / 2 , str(count), ha='right', va='center')\n",
    "\n",
    "# Add value labels to each bar for reviews\n",
    "for count, bar in zip(counts_reviews, bars_reviews):\n",
    "    ax.text(bar.get_width() + 48 , bar.get_y() + bar.get_height() / 2 , str(count), ha='right', va='center')\n",
    "\n",
    "# Set the y-axis ticks and labels\n",
    "ax.set_yticks(ind + bar_width / 2)\n",
    "ax.set_yticklabels(apps_unique_words)\n",
    "\n",
    "plt.xlabel('Number of', fontsize=12)  # Increase font size to 12\n",
    "plt.ylabel('Apps', fontsize=12)  # Increase font size to 12\n",
    "\n",
    "# Increase font size of tick labels on both axes\n",
    "ax.tick_params(axis='x', labelsize=12)  # Increase x-axis tick label font size to 10\n",
    "ax.tick_params(axis='y', labelsize=12)  # Increase y-axis tick label font size to 10\n",
    "\n",
    "# Set the chart title\n",
    "#ax.set_title('Number of Unique Words and Reviews for Each App')\n",
    "\n",
    "# Set the legend\n",
    "ax.legend()\n",
    "\n",
    "# Display the chart\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137b5fa0",
   "metadata": {},
   "source": [
    "### Remove some apps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da987057",
   "metadata": {},
   "outputs": [],
   "source": [
    "del app_corpus_subdfs['googlehome']\n",
    "del app_corpus_subdfs['zoom']\n",
    "del app_corpus_subdfs['linkedin']\n",
    "del app_corpus_subdfs['instagram']\n",
    "del app_corpus_subdfs['alexa']\n",
    "del app_corpus_subdfs['vinted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2cecbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_corpus_subdfs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca4e7e3",
   "metadata": {},
   "source": [
    "### Frequency of the words & Support metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28674a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "te = TransactionEncoder()\n",
    "app_word_counts_subdfs = {}\n",
    "app_support_counts_subdfs = {}\n",
    "for app in app_corpus_subdfs.keys():\n",
    "    word_counts = app_corpus_subdfs[app].sum()\n",
    "# Create a new dataframe to store the word counts\n",
    "    word_counts_df = pd.DataFrame({'Word': word_counts.index, 'Count': word_counts.values})\n",
    "\n",
    "# Calculate the total number of transactions\n",
    "    total_transactions = len(app_corpus_subdfs[app].index)\n",
    "# Calculate the support for each word\n",
    "    word_counts_df['Support'] = word_counts_df['Count'] / total_transactions\n",
    "    \n",
    "# Sort the dataframe by the word support in descending order\n",
    "    word_counts_df = word_counts_df.sort_values('Support', ascending=False)\n",
    "    app_word_counts_subdfs[app] = word_counts_df\n",
    "    #print(app_word_counts_subdfs)\n",
    "    support_counts = word_counts_df.groupby(word_counts_df['Support'].round(3))['Word'].nunique().reset_index()\n",
    "    app_support_counts_subdfs[app] = support_counts\n",
    "    #print(app_support_counts_subdfs)\n",
    "\n",
    "\n",
    "# Print the DataFrame with word counts and their respective supports\n",
    "#word_counts_df\n",
    "#app_support_counts_subdfs['youtube']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b871dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the number of unique words for each support value\n",
    "import matplotlib.ticker as mticker\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "color_palette = sns.color_palette(\"tab20\", len(app_support_counts_subdfs.keys()))\n",
    "for app, color in zip(app_support_counts_subdfs.keys(), color_palette):\n",
    "    app_df = app_support_counts_subdfs[app]\n",
    "    plt.plot(app_df['Support'], app_df['Word'], color=color,  label=app)\n",
    "plt.xlabel('Support')\n",
    "plt.ylabel('Number of unique words')\n",
    "#plt.title('Number of Unique Words for Support Value')\n",
    "plt.xticks(minor=True)\n",
    "#plt.yscale('log')\n",
    "# Format x-axis as percentages\n",
    "#plt.gca().xaxis.set_major_formatter('{:.3f}'.format)\n",
    "plt.xlim(0,0.15)\n",
    "plt.legend(title='Apps')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d92827",
   "metadata": {},
   "source": [
    "### Finding frequent itemsets with Apriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd06321",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets_subdfs = {}\n",
    "start_time = time.time()\n",
    "# max_len = 2 could be used to get only top rules\n",
    "support_app = [0.02, 0.04, 0.04, 0.04]\n",
    "#for app in app_corpus_subdfs:\n",
    "for app, support in zip(app_corpus_subdfs, support_app):\n",
    "    print(app, support, len(app_corpus_subdfs[app]))\n",
    "    frequent_itemsets = apriori(app_corpus_subdfs[app], min_support = support, use_colnames=True, low_memory=True)\n",
    "    ## 0.001 rumtime error\n",
    "\n",
    "    frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
    "    print(\"the number of frequent itemsets generated:\", len(frequent_itemsets))\n",
    "    #frequent_itemsets = frequent_itemsets[frequent_itemsets['length']> 1]\n",
    "    frequent_itemsets_subdfs[app] = frequent_itemsets\n",
    "print(\"---Runtime: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582cff05",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets_counts_subdfs = {}\n",
    "for app in frequent_itemsets_subdfs:\n",
    "    frequent_itemsets_counts = frequent_itemsets_subdfs[app].groupby(['length']).size().reset_index(name='no. itemsets')\n",
    "    frequent_itemsets_counts_subdfs[app] = frequent_itemsets_counts\n",
    "    \n",
    "# Convert dictionary to DataFrame\n",
    "frequent_itemsets_all_df = pd.concat({k: pd.DataFrame(v) for k, v in frequent_itemsets_counts_subdfs.items()}, axis=0)\n",
    "\n",
    "# Reset index\n",
    "frequent_itemsets_all_df.reset_index(level=1, inplace=True)\n",
    "frequent_itemsets_all_df.rename(columns={'level_1': 'app'}, inplace=True)\n",
    "frequent_itemsets_all_df.to_csv('frequent_itemsets_counts.csv')\n",
    "frequent_itemsets_all_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962f529d",
   "metadata": {},
   "source": [
    "### APRIORI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0a514b",
   "metadata": {},
   "outputs": [],
   "source": [
    "support_app = [0.02, 0.04, 0.04, 0.04]\n",
    "for app, support in zip(app_corpus_subdfs, support_app):\n",
    "    print(app)\n",
    "    %timeit -n 100 -r 10 apriori(app_corpus_subdfs[app], min_support=support, low_memory=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fc455b",
   "metadata": {},
   "source": [
    "### FP-GROWTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc0b7416",
   "metadata": {},
   "outputs": [],
   "source": [
    "support_app = [0.02, 0.04, 0.04, 0.04]\n",
    "from mlxtend.frequent_patterns import fpgrowth\n",
    "for app, support in zip(app_corpus_subdfs, support_app):\n",
    "    print(app)\n",
    "    %timeit -n 100 -r 10 fpgrowth(app_corpus_subdfs[app], min_support=support)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
