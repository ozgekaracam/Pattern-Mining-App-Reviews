{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4da05e37",
   "metadata": {},
   "source": [
    "# NLP - Cleaning and Preprocessing Text Data of User Reviews in AppStore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac67a4c4",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f86a077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas\n",
    "import pandas as pd\n",
    "# natural language toolkit\n",
    "import nltk\n",
    "# string for punctuation list\n",
    "import string\n",
    "# to remove links, numbers\n",
    "import re\n",
    "# to get stopwords from smart stopword list link\n",
    "from urllib.request import urlopen\n",
    "# wordnet for part of the speech\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "# Tokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "# Lemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#Stemmers\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a7341d",
   "metadata": {},
   "source": [
    "##  CSV Read and DataFrame Creation\n",
    "\n",
    "We load a CSV file, create a DataFrame, and verify its shape. Initially, we have a dataset with 3097 rows and 16 columns, where each row represents a distinct reviews posted on AppStore for 10 different apps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7ded14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(file):\n",
    "    data = pd.read_csv(file)\n",
    "    print(data.shape)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d02253",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"gps_reannotation-full.csv\"\n",
    "df = get_data(file)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a62398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique values of apps and raised ethical concerns of reviews\n",
    "apps = df['app_name'].unique()\n",
    "print('Apps:', ', '.join(apps))\n",
    "\n",
    "concerns = df['cat1'].unique()\n",
    "print('\\nRaised Ethical Concerns: ', ', '.join(concerns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4fd8f0",
   "metadata": {},
   "source": [
    "## Remove links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02c74a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeLink(text):\n",
    "    no_link = ' '.join(re.sub(\"(w+://S+)\", \" \", text).split())\n",
    "    return no_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafcd6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_content'] = df['content'].apply(lambda x: removeLink(x))\n",
    "df['clean_content']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c765aea",
   "metadata": {},
   "source": [
    "## Remove numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b4b7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeNumber(text):\n",
    "    return ' '.join(re.sub(r'[0-9]',' ', text).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdf0cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_content'] = df['clean_content'].apply(lambda x: removeNumber(x))\n",
    "\n",
    "df['clean_content']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6723b9",
   "metadata": {},
   "source": [
    "## Remove Emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e026f425",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deEmojify(text):\n",
    "    return text.encode('ascii', 'ignore').decode('ascii')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dce92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_content'] = df['clean_content'].apply(lambda x: deEmojify(x))\n",
    "\n",
    "#df['clean_content']\n",
    "print(df.loc[450, ['content','clean_content']].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2023f3",
   "metadata": {},
   "source": [
    "## Converting all characters to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9bc47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_content'] = df['clean_content'].apply(lambda x: x.lower())\n",
    "df['clean_content']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094c95fb",
   "metadata": {},
   "source": [
    "## Remove stopwords\n",
    "* nltk.corpus.stopwords.words('english') could be also used. However, it contains 179, whereas smart stopword list does 571 words, including ‘i’, ‘me’, ‘my’, ‘myself’, ‘we’, ‘you’, ‘he’, ‘his’, for instance. \n",
    "* stpwrd is here extended with app names that are mentioned in the reviews as well since they are going to be included in every reviews that belong to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e2b7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_stopwords():\n",
    "    stpwrd_url = \"http://www.ai.mit.edu/projects/jmlr/papers/volume5/lewis04a/a11-smart-stop-list/english.stop\"\n",
    "    response = urlopen(stpwrd_url)\n",
    "    stpwrds = response.read().decode('utf-8').replace(\"\\n\", \" \").split()\n",
    "    return stpwrds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62478259",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text, stpwrds):\n",
    "    text = text.split(\" \")\n",
    "    words = [w for w in text if w not in stpwrds]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7749be33",
   "metadata": {},
   "outputs": [],
   "source": [
    "stpwrds = generate_stopwords()\n",
    "df['clean_content'] = df['clean_content'].apply(lambda x: remove_stopwords(x, stpwrds))\n",
    "df['clean_content'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5972e487",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['clean_content']\n",
    "print(df.loc[400, ['content','clean_content']].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75a0361f",
   "metadata": {},
   "source": [
    "## Remove punctuation\n",
    "The process of punctuation elimination involves iterating through the series using list comprehension and preserving all elements that do not exist in the __string.punctuation__ list. This list, imported at the beginning using __import string__, comprises all punctuation marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c665af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def removePunctuation(text):\n",
    "    no_punc = \"\".join([c for c in text if c not in string.punctuation])\n",
    "    return no_punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b4e5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_content'] = df['clean_content'].apply(lambda x: removePunctuation(x))\n",
    "df['clean_content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cd9c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_content'] = df['clean_content'].apply(lambda x: remove_stopwords(x, stpwrds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28600e28",
   "metadata": {},
   "source": [
    "## Tokenizing words\n",
    "\n",
    "* __RegexpTokenizer__ is a function that is used to break down a string into smaller substrings based on a specified regular expression pattern. The selected pattern splits up by spaces that are not attached to a digit as numbers are already cleaned from reviews.\n",
    "* __discard\\_empty__ is set to True. It ensures that any empty tokens produced by the tokenizer are removed from the resulting output. \n",
    "(see in https://www.nltk.org/_modules/nltk/tokenize/regexp.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc02e1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+|\\$[\\d\\.]+|\\S+', discard_empty=True)\n",
    "df['clean_content'] = df['clean_content'].apply(lambda x: tokenizer.tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7497c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['clean_content'])\n",
    "print(\"\\nOne particular review:\")\n",
    "print(df.loc[400, ['content','clean_content']].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70d3733",
   "metadata": {},
   "source": [
    "## Lemmatizing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6612c2b6",
   "metadata": {},
   "source": [
    "#### WordNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d51d04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_part_of_speech(word):\n",
    "    probable_part_of_speech = wordnet.synsets(word)\n",
    "    pos_counts = Counter()\n",
    "    #pos_counts[\"n\"] = len([item for item in probable_part_of_speech if item.pos() == \"n\"])\n",
    "    pos_counts[\"v\"] = len([item for item in probable_part_of_speech if item.pos() == \"v\"])\n",
    "    #pos_counts[\"n\"] = len([item for item in probable_part_of_speech if item.pos() == \"n\"])\n",
    "    pos_counts[\"a\"] = len([item for item in probable_part_of_speech if item.pos() == \"a\"])  \n",
    "    pos_counts[\"n\"] = len([item for item in probable_part_of_speech if item.pos() == \"n\"])\n",
    "    #pos_counts[\"r\"] = len([item for item in probable_part_of_speech if item.pos() == \"r\"])\n",
    "\n",
    "    most_likely_part_of_speech = pos_counts.most_common(1)[0][0]\n",
    "    return most_likely_part_of_speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edda441",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_lemmatizer(text, lemmatizer):\n",
    "    lem_text = [lemmatizer.lemmatize(i, get_part_of_speech(i)) for i in text]\n",
    "    return lem_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261a08f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnetlemma =  WordNetLemmatizer()\n",
    "df['clean_content'] = df['clean_content'].apply(lambda x: word_lemmatizer(x, wordnetlemma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574198ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[['content','clean_content']])\n",
    "print(\"\\nOne particular review:\")\n",
    "print(df.loc[0, ['content','clean_content']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba5782b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_content'] = [' '.join(x) for x in df['clean_content']]\n",
    "df['clean_content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8df4752",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cat1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bdd91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cat1_clean'] = df['cat1'].apply(lambda x: x.lower())\n",
    "df['cat1_clean'] = df['cat1_clean'].str.extract(r'^(.*?)\\(', expand=True)\n",
    "df['cat1_clean'].fillna(df['cat1'], inplace=True)\n",
    "df['cat1_clean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ae6967",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cat1_clean'] = df['cat1_clean'].str.strip()\n",
    "\n",
    "df['cat1'] = df['cat1_clean']\n",
    "df['cat1'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d806c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e447442",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.loc[400, 'content'])\n",
    "print(df.loc[400, 'clean_content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51449f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
