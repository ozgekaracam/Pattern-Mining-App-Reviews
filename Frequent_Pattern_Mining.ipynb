{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9411bf0d",
   "metadata": {},
   "source": [
    "  # Frequent Pattern Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b036e471",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244376ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f168c6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95a9c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import fpgrowth\n",
    "from mlxtend.frequent_patterns import association_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1225df11",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r predict_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a1d745",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_df = predict_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560abe68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_appwords():\n",
    "    stpwrds = [\"app\", \"alexa\", \"facebook\", \n",
    "                     \"googlehome\", \"instagram\", \"linkedin\", \"tiktok\", \"tik\", \"tok\", \"uber\", \"youtube\", \"fb\", \n",
    "               \"dont\", \"yall\", \"kinda\", \"lot\", \"anymore\", \"doesnt\", \"tube\", \"blm\", \"thing\"]\n",
    "    return stpwrds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a07d8c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(text, stpwrds):\n",
    "    #text = text.split(\" \")\n",
    "    words = [w for w in text if w not in stpwrds]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758342dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "stpwrds = generate_appwords()\n",
    "predict_df['clean_content'] = predict_df['clean_content'].apply(lambda x: remove_stopwords(x, stpwrds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b29acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_df['clean_content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c35be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dictionary to store the sub dataframes for each app\n",
    "app_subdfs = {}\n",
    "\n",
    "# Iterate over each unique app name\n",
    "for app in predict_df['app_name'].unique():\n",
    "    # Filter the dataframe for the current app\n",
    "    sub_df = predict_df[predict_df['app_name'] == app]\n",
    "    # Store the sub dataframe in the dictionary with the app name as the key\n",
    "    app_subdfs[app] = sub_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55326783",
   "metadata": {},
   "source": [
    "### One-hot encoding transaction data\n",
    "corpus_list here transformed into a one-hot encoded data frame, where each column consists of true and false values that indicate whether a word was included in a review. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2c2249",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_corpus_list = {}\n",
    "def create_corpus(df,  concern: bool = False):\n",
    "    for app in app_subdfs.keys():\n",
    "        if concern:\n",
    "            for index, row in app_subdfs[app].iterrows():\n",
    "                row[\"clean_content\"].append(row[\"predicted\"])\n",
    "        corpus_list = app_subdfs[app][\"clean_content\"].tolist()\n",
    "        app_corpus_list[app] = corpus_list\n",
    "    return app_corpus_list\n",
    "\n",
    "app_corpus_list = create_corpus(app_subdfs, concern=True)\n",
    "#app_corpus_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1780c96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dictionary to store the sub dataframes for each app\n",
    "app_corpus_subdfs = {}\n",
    "te = TransactionEncoder()\n",
    "for app in app_subdfs.keys():\n",
    "    print(app)\n",
    "    te_ary = te.fit(app_corpus_list[app]).transform(app_corpus_list[app])\n",
    "    corpus_df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "    app_corpus_subdfs[app] = corpus_df\n",
    "    print(corpus_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbb5e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a list to store the number of unique words for each app\n",
    "unique_words_app = {}\n",
    "\n",
    "# Iterate over the sub-dataframes\n",
    "for app, sub_df in app_corpus_subdfs.items():\n",
    "    unique_words_app[app] = len(sub_df.columns)\n",
    "unique_words_app = sorted(unique_words_app.items(), key=lambda x: x[1], reverse=True)\n",
    "unique_words_app = dict(unique_words_app)\n",
    "# Sort the unique_words_app dictionary by values in descending order\n",
    "unique_words_app_list = sorted(unique_words_app.items(), key=lambda x: x[1], reverse=False)\n",
    "\n",
    "# Create a list to store the number of reviews for each app\n",
    "no_review_app = {}\n",
    "\n",
    "# Iterate over the sub-dataframes\n",
    "for app, sub_df in app_corpus_subdfs.items():\n",
    "    no_review_app[app] = len(sub_df.index)\n",
    "no_review_app = sorted(no_review_app.items(), key=lambda x: x[1], reverse=True)\n",
    "no_review_app = dict(no_review_app)\n",
    "# Sort the no_review_app dictionary by values in descending order\n",
    "no_review_app_list = sorted(no_review_app.items(), key=lambda x: x[1], reverse=False)\n",
    "\n",
    "# Extract the sorted apps and counts for unique words\n",
    "apps_unique_words = [app for app, count in unique_words_app_list]\n",
    "counts_unique_words = [count for app, count in unique_words_app_list]\n",
    "\n",
    "# Extract the sorted apps and counts for reviews\n",
    "apps_reviews = [app for app, count in no_review_app_list]\n",
    "counts_reviews = [count for app, count in no_review_app_list]\n",
    "\n",
    "# Set the width of the bars\n",
    "bar_width = 0.45\n",
    "\n",
    "# Create an array of indices for the x-axis ticks\n",
    "ind = np.arange(len(apps_unique_words))\n",
    "\n",
    "# Create the figure and axes\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Plot the bars for unique words\n",
    "bars_unique_words = ax.barh(ind, counts_unique_words, bar_width, color='#1f77b4', label='Unique Words')\n",
    "\n",
    "# Plot the bars for reviews\n",
    "bars_reviews = ax.barh(ind + bar_width, counts_reviews, bar_width, color='#0f3652', label='Reviews')\n",
    "\n",
    "# Add value labels to each bar for unique words\n",
    "for count, bar in zip(counts_unique_words, bars_unique_words):\n",
    "    ax.text(bar.get_width() + 50 , bar.get_y() + bar.get_height() / 2 , str(count), ha='right', va='center', fontsize=12)\n",
    "\n",
    "# Add value labels to each bar for reviews\n",
    "for count, bar in zip(counts_reviews, bars_reviews):\n",
    "    ax.text(bar.get_width() + 50 , bar.get_y() + bar.get_height() / 2 , str(count), ha='right', va='center', fontsize=12)\n",
    "\n",
    "# Set the y-axis ticks and labels\n",
    "ax.set_yticks(ind + bar_width / 2)\n",
    "ax.set_yticklabels(apps_unique_words)\n",
    "\n",
    "plt.xlabel('Count', fontsize=13)  # Increase font size to 12\n",
    "plt.ylabel('Apps', fontsize=13)  # Increase font size to 12\n",
    "\n",
    "# Increase font size of tick labels on both axes\n",
    "ax.tick_params(axis='x', labelsize=13)  # Increase x-axis tick label font size to 10\n",
    "ax.tick_params(axis='y', labelsize=13)  # Increase y-axis tick label font size to 10\n",
    "\n",
    "# Set the chart title\n",
    "#ax.set_title('Number of Unique Words and Reviews for Each App')\n",
    "plt.xlim(0,899)\n",
    "# Set the legend\n",
    "ax.legend()\n",
    "\n",
    "# Display the chart\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137b5fa0",
   "metadata": {},
   "source": [
    "### Remove some apps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da987057",
   "metadata": {},
   "outputs": [],
   "source": [
    "del app_corpus_subdfs['googlehome']\n",
    "del app_corpus_subdfs['zoom']\n",
    "del app_corpus_subdfs['linkedin']\n",
    "del app_corpus_subdfs['instagram']\n",
    "del app_corpus_subdfs['alexa']\n",
    "del app_corpus_subdfs['vinted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2cecbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_corpus_subdfs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fa9622",
   "metadata": {},
   "outputs": [],
   "source": [
    "app_corpus_subdfs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca4e7e3",
   "metadata": {},
   "source": [
    "### Frequency of the words & Support metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28674a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "te = TransactionEncoder()\n",
    "app_word_counts_subdfs = {}\n",
    "app_support_counts_subdfs = {}\n",
    "for app in app_corpus_subdfs.keys():\n",
    "    word_counts = app_corpus_subdfs[app].sum()\n",
    "# Create a new dataframe to store the word counts\n",
    "    word_counts_df = pd.DataFrame({'Word': word_counts.index, 'Count': word_counts.values})\n",
    "\n",
    "# Calculate the total number of transactions\n",
    "    total_transactions = len(app_corpus_subdfs[app].index)\n",
    "# Calculate the support for each word\n",
    "    word_counts_df['Support'] = word_counts_df['Count'] / total_transactions\n",
    "    \n",
    "# Sort the dataframe by the word support in descending order\n",
    "    word_counts_df = word_counts_df.sort_values('Support', ascending=False)\n",
    "    app_word_counts_subdfs[app] = word_counts_df\n",
    "    #print(app_word_counts_subdfs)\n",
    "    support_counts = word_counts_df.groupby(word_counts_df['Support'].round(3))['Word'].nunique().reset_index()\n",
    "    app_support_counts_subdfs[app] = support_counts\n",
    "    #print(app_support_counts_subdfs)\n",
    "\n",
    "\n",
    "# Print the DataFrame with word counts and their respective supports\n",
    "#word_counts_df\n",
    "#app_support_counts_subdfs['youtube']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b871dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the number of unique words for each support value\n",
    "import matplotlib.ticker as mticker\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "color_palette = sns.color_palette(\"tab20\", len(app_support_counts_subdfs.keys()))\n",
    "for app, color in zip(app_support_counts_subdfs.keys(), color_palette):\n",
    "    app_df = app_support_counts_subdfs[app]\n",
    "    plt.plot(app_df['Support'], app_df['Word'], color=color,  label=app)\n",
    "\n",
    "plt.xlabel('Support', fontsize=12)  # Increase font size to 12\n",
    "plt.ylabel('Number of unique words', fontsize=12)  # Increase font size to 12\n",
    "\n",
    "# Increase font size of tick labels on both axes\n",
    "ax.tick_params(axis='x', labelsize=14)  # Increase x-axis tick label font size to 10\n",
    "ax.tick_params(axis='y', labelsize=14)  # Increase y-axis tick label font size to 10\n",
    "#plt.title('Number of Unique Words for Support Value')\n",
    "#plt.xticks(minor=True)\n",
    "#plt.yscale('log')\n",
    "# Format x-axis as percentages\n",
    "#plt.gca().xaxis.set_major_formatter('{:.3f}'.format)\n",
    "plt.xlim(0,0.15)\n",
    "plt.legend(title='Apps')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d92827",
   "metadata": {},
   "source": [
    "### Finding frequent itemsets with Apriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd06321",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets_subdfs = {}\n",
    "start_time = time.time()\n",
    "# max_len = 2 could be used to get only top rules\n",
    "support_app = [0.02, 0.04, 0.04, 0.04]\n",
    "#for app in app_corpus_subdfs:\n",
    "for app, support in zip(app_corpus_subdfs, support_app):\n",
    "    print(app, support, len(app_corpus_subdfs[app]))\n",
    "    frequent_itemsets = apriori(app_corpus_subdfs[app], min_support = support, use_colnames=True, max_len=2, low_memory=True)\n",
    "    ## 0.001 rumtime error\n",
    "\n",
    "    frequent_itemsets['length'] = frequent_itemsets['itemsets'].apply(lambda x: len(x))\n",
    "    print(\"the number of frequent itemsets generated:\", len(frequent_itemsets))\n",
    "    #frequent_itemsets = frequent_itemsets[frequent_itemsets['length']> 1]\n",
    "    frequent_itemsets_subdfs[app] = frequent_itemsets\n",
    "print(\"---Runtime: %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdc46c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets_counts_subdfs = {}\n",
    "for app in frequent_itemsets_subdfs:\n",
    "    frequent_itemsets_counts = frequent_itemsets_subdfs[app].groupby(['length']).size().reset_index(name='no. itemsets')\n",
    "    frequent_itemsets_counts_subdfs[app] = frequent_itemsets_counts\n",
    "    \n",
    "# Convert dictionary to DataFrame\n",
    "frequent_itemsets_all_df = pd.concat({k: pd.DataFrame(v) for k, v in frequent_itemsets_counts_subdfs.items()}, axis=0)\n",
    "\n",
    "# Reset index\n",
    "frequent_itemsets_all_df.reset_index(level=1, inplace=True)\n",
    "frequent_itemsets_all_df.rename(columns={'level_1': 'app'}, inplace=True)\n",
    "frequent_itemsets_all_df.to_csv('frequent_itemsets_counts.csv')\n",
    "frequent_itemsets_all_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9fd634",
   "metadata": {},
   "source": [
    "### Generating association rules\n",
    "Association rules are genenrated with no additional pruning for now at this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399622b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate association rules without performing additional pruning\n",
    "rules_subdfs = {}\n",
    "for app in frequent_itemsets_subdfs:\n",
    "    rules = association_rules(frequent_itemsets_subdfs[app], metric='support', min_threshold = 0.0000000)\n",
    "    print(app, len(rules))\n",
    "    rules_subdfs[app] = rules\n",
    "#rules_subdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0ac427",
   "metadata": {},
   "outputs": [],
   "source": [
    "for app in rules_subdfs.keys():\n",
    "    print(app)\n",
    "    rules_subdfs[app].describe().to_csv(f'rules_description_two_itemsets{app}.csv')\n",
    "    print(len(rules_subdfs[app]))\n",
    "    \n",
    "    #print(rules_subdfs[app].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35453035",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "sns.scatterplot(ax=axes[0, 0], x=\"antecedent support\", y=\"consequent support\", size=\"confidence\", hue=\"confidence\",\n",
    "                data=rules_subdfs['tiktok'], palette=sns.color_palette(\"ch:start=.2,rot=-.3\", as_cmap=True), s=50)\n",
    "axes[0, 0].set_xlabel(\"antecedent support\", fontsize=14)\n",
    "axes[0, 0].set_ylabel(\"consequent support\", fontsize=14)\n",
    "axes[0, 0].tick_params(axis='both', which='major', labelsize=14)  # Adjust font size of tick labels\n",
    "\n",
    "sns.scatterplot(ax=axes[0, 1], x=\"antecedent support\", y=\"consequent support\", size=\"confidence\", hue=\"confidence\",\n",
    "                data=rules_subdfs['facebook'], palette=sns.color_palette(\"ch:start=.2,rot=-.3\", as_cmap=True),\n",
    "                s=50)\n",
    "axes[0, 1].set_xlabel(\"antecedent support\", fontsize=14)\n",
    "axes[0, 1].set_ylabel(\"consequent support\", fontsize=14)\n",
    "axes[0, 1].tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "sns.scatterplot(ax=axes[1, 0], x=\"antecedent support\", y=\"consequent support\", size=\"confidence\", hue=\"confidence\",\n",
    "                data=rules_subdfs['uber'], palette=sns.color_palette(\"ch:start=.2,rot=-.3\", as_cmap=True),\n",
    "                s=50)\n",
    "axes[1, 0].set_xlabel(\"antecedent support\", fontsize=14)\n",
    "axes[1, 0].set_ylabel(\"consequent support\", fontsize=14)\n",
    "axes[1, 0].tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "sns.scatterplot(ax=axes[1, 1], x=\"antecedent support\", y=\"consequent support\", size=\"confidence\", hue=\"confidence\",\n",
    "                data=rules_subdfs['youtube'], palette=sns.color_palette(\"ch:start=.2,rot=-.3\", as_cmap=True),\n",
    "                s=50)\n",
    "axes[1, 1].set_xlabel(\"antecedent support\", fontsize=14)\n",
    "axes[1, 1].set_ylabel(\"consequent support\", fontsize=14)\n",
    "axes[1, 1].tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f9731b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate association rules without performing additional pruning\n",
    "for app in rules_subdfs.keys():\n",
    "    print(app, len(rules_subdfs[app]))\n",
    "    rules = rules_subdfs[app][rules_subdfs[app]['antecedent support'] <= rules_subdfs[app]['consequent support']]\n",
    "    rules_subdfs[app] = rules\n",
    "    print('greater consequent support than antecedent support', app, len(rules_subdfs[app]))\n",
    "#rules_subdfs  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b84b52e",
   "metadata": {},
   "source": [
    "### Optimality of the support-confidence border"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2e3579",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "sns.scatterplot(ax=axes[0, 0], x=\"support\", y=\"confidence\", size=\"lift\", hue=\"lift\",\n",
    "                data=rules_subdfs['tiktok'], palette=sns.color_palette(\"ch:start=.2,rot=-.3\", as_cmap=True))\n",
    "axes[0, 0].set_xlabel(\"support\", fontsize=14)\n",
    "axes[0, 0].set_ylabel(\"confidence\", fontsize=14)\n",
    "axes[0, 0].tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "sns.scatterplot(ax=axes[0, 1], x=\"support\", y=\"confidence\", size=\"lift\", hue=\"lift\",\n",
    "                data=rules_subdfs['facebook'], palette=sns.color_palette(\"ch:start=.2,rot=-.3\", as_cmap=True))\n",
    "axes[0, 1].set_xlabel(\"support\", fontsize=14)\n",
    "axes[0, 1].set_ylabel(\"confidence\", fontsize=14)\n",
    "axes[0, 1].tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "sns.scatterplot(ax=axes[1, 0], x=\"support\", y=\"confidence\", size=\"lift\", hue=\"lift\",\n",
    "                data=rules_subdfs['uber'], palette=sns.color_palette(\"ch:start=.2,rot=-.3\", as_cmap=True))\n",
    "axes[1, 0].set_xlabel(\"support\", fontsize=14)\n",
    "axes[1, 0].set_ylabel(\"confidence\", fontsize=14)\n",
    "axes[1, 0].tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "sns.scatterplot(ax=axes[1, 1], x=\"support\", y=\"confidence\", size=\"lift\", hue=\"lift\",\n",
    "                data=rules_subdfs['youtube'], palette=sns.color_palette(\"ch:start=.2,rot=-.3\", as_cmap=True))\n",
    "axes[1, 1].set_xlabel(\"support\", fontsize=14)\n",
    "axes[1, 1].set_ylabel(\"confidence\", fontsize=14)\n",
    "axes[1, 1].tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a783a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "sns.scatterplot(ax=axes[0, 0], x=\"zhangs_metric\", y=\"lift\", color='#1f77b4', data=rules_subdfs['tiktok'])\n",
    "axes[0, 0].set_xlabel('zhang', fontsize=14)\n",
    "axes[0, 0].set_ylabel('lift', fontsize=14)\n",
    "axes[0, 0].tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "sns.scatterplot(ax=axes[0, 1], x=\"zhangs_metric\", y=\"lift\", color='#1f77b4', data=rules_subdfs['facebook'])\n",
    "axes[0, 1].set_xlabel('zhang', fontsize=14)\n",
    "axes[0, 1].set_ylabel('lift', fontsize=14)\n",
    "axes[0, 1].tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "sns.scatterplot(ax=axes[1, 0], x=\"zhangs_metric\", y=\"lift\", color='#1f77b4', data=rules_subdfs['uber'])\n",
    "axes[1, 0].set_xlabel('zhang', fontsize=14)\n",
    "axes[1, 0].set_ylabel('lift', fontsize=14)\n",
    "axes[1, 0].tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "sns.scatterplot(ax=axes[1, 1], x=\"zhangs_metric\", y=\"lift\", color='#1f77b4', data=rules_subdfs['youtube'])\n",
    "axes[1, 1].set_xlabel('zhang', fontsize=14)\n",
    "axes[1, 1].set_ylabel('lift', fontsize=14)\n",
    "axes[1, 1].tick_params(axis='both', which='major', labelsize=14)\n",
    "\n",
    "fig.suptitle('Relationship Between Support and Confidence in Association Rules', fontsize=16)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85790248",
   "metadata": {},
   "source": [
    "## Disassociation rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fd76e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rules_subdfs['tiktok'][rules_subdfs['tiktok']['antecedent support'] <= rules_subdfs['tiktok']['consequent support']]\n",
    "#rules_subdfs['tiktok'][rules_subdfs['tiktok']['zhangs_metric'] <= 0] \n",
    "#rules_subdfs['facebook'][rules_subdfs['facebook']['zhangs_metric'] <= 0]  \n",
    "#rules_subdfs['uber'][rules_subdfs['uber']['zhangs_metric'] <= 0]  \n",
    "#rules_subdfs['youtube'][rules_subdfs['youtube']['zhangs_metric'] <= 0] \n",
    "\n",
    "for app in rules_subdfs.keys():\n",
    "    print(app)\n",
    "    rules_subdfs[app][rules_subdfs[app]['zhangs_metric'] <= 0].to_csv(f'disassociation_rules_{app}.csv')\n",
    "    print(len(rules_subdfs[app][rules_subdfs[app]['zhangs_metric'] <= 0]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38694f0f",
   "metadata": {},
   "source": [
    "# Strong Association Rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e230cf65",
   "metadata": {},
   "source": [
    "## TikTok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceaea491",
   "metadata": {},
   "outputs": [],
   "source": [
    "strong_rules_tiktok = rules_subdfs['tiktok'][(rules_subdfs['tiktok']['zhangs_metric'] >= 0.5) & \n",
    "                                               (rules_subdfs['tiktok']['lift'] >= 2) &\n",
    "                                            (rules_subdfs['tiktok']['confidence'] >= 0.4)]\n",
    "print('All rules: ', len(rules_subdfs['tiktok']))\n",
    "print('Strong rules: ', len(strong_rules_tiktok))\n",
    "\n",
    "strong_rules_tiktok .sort_values('zhangs_metric', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb2824c",
   "metadata": {},
   "source": [
    "## Facebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88039fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "strong_rules_facebook = rules_subdfs['facebook'][(rules_subdfs['facebook']['zhangs_metric'] >= 0.4) &\n",
    "                                                 (rules_subdfs['facebook']['lift'] >= 2)&\n",
    "                                                 (rules_subdfs['facebook']['confidence'] >= 0.3)]\n",
    "print('All rules: ', len(rules_subdfs['facebook']))\n",
    "print('Strong rules: ', len(strong_rules_facebook))\n",
    "\n",
    "strong_rules_facebook.sort_values('confidence', ascending=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8facf3",
   "metadata": {},
   "source": [
    "## Uber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48138ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "strong_rules_uber = rules_subdfs['uber'][(rules_subdfs['uber']['zhangs_metric'] >= 0.4) & \n",
    "                                         (rules_subdfs['uber']['confidence'] >= 0.3) & \n",
    "                                               (rules_subdfs['uber']['lift'] >= 2)]\n",
    "print('All rules: ', len(rules_subdfs['uber']))\n",
    "print('Strong rules: ', len(strong_rules_uber))\n",
    "\n",
    "strong_rules_uber.sort_values('confidence', ascending=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325e2e7d",
   "metadata": {},
   "source": [
    "## Youtube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7210fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "strong_rules_youtube = rules_subdfs['youtube'][(rules_subdfs['youtube']['zhangs_metric'] >= 0.7) & \n",
    "                                               (rules_subdfs['youtube']['confidence'] >= 0.5) & \n",
    "                                               (rules_subdfs['youtube']['lift'] >= 2)]\n",
    "                                               \n",
    "print('All rules: ', len(rules_subdfs['youtube']))\n",
    "print('Strong rules: ', len(strong_rules_youtube))\n",
    "\n",
    "strong_rules_youtube.sort_values('zhangs_metric', ascending=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb799d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "strong_rules_tiktok.antecedents = strong_rules_tiktok.antecedents.apply(lambda x: next(iter(x)))\n",
    "strong_rules_tiktok.consequents = strong_rules_tiktok.consequents.apply(lambda x: next(iter(x)))\n",
    "strong_rules_tiktok.to_csv('strong_rules_tiktok.csv')\n",
    "\n",
    "strong_rules_facebook.antecedents = strong_rules_facebook.antecedents.apply(lambda x: next(iter(x)))\n",
    "strong_rules_facebook.consequents = strong_rules_facebook.consequents.apply(lambda x: next(iter(x)))\n",
    "strong_rules_facebook.to_csv('strong_rules_facebook.csv')\n",
    "\n",
    "strong_rules_uber.antecedents = strong_rules_uber.antecedents.apply(lambda x: next(iter(x)))\n",
    "strong_rules_uber.consequents = strong_rules_uber.consequents.apply(lambda x: next(iter(x)))\n",
    "strong_rules_uber.to_csv('strong_rules_uber.csv')\n",
    "\n",
    "strong_rules_youtube.antecedents = strong_rules_youtube.antecedents.apply(lambda x: next(iter(x)))\n",
    "strong_rules_youtube.consequents = strong_rules_youtube.consequents.apply(lambda x: next(iter(x)))\n",
    "strong_rules_youtube.to_csv('strong_rules_youtube.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
